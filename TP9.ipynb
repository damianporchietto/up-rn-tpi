{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e0d5df",
   "metadata": {},
   "source": [
    "# Trabajo Práctico – Comparación de Autoencoders para Clasificación de Géneros Musicales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516a4a07",
   "metadata": {},
   "source": [
    "\n",
    "**Objetivo**  \n",
    "Comparar el desempeño de clasificadores basados en *modelos autoasociativos* (autoencoders) para la extracción de características, a partir de un conjunto de datos con más de 500 variables derivadas del análisis de audio.\n",
    "\n",
    "Se implementarán dos variantes de autoencoder:\n",
    "\n",
    "* Auto‑Encoder Estándar (AE)  \n",
    "* Denoising Auto‑Encoder (DAE)\n",
    "\n",
    "Luego se utilizará la representación latente de cada modelo para entrenar un clasificador *Perceptrón Multicapa* (MLP) que distinga **10 géneros musicales**.\n",
    "\n",
    "La implementación se realiza íntegramente en **PyTorch**, con el apoyo de Scikit‑Learn y pandas para el manejo de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6739fa9a",
   "metadata": {},
   "source": [
    "## 1. Carga de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28761c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf283d68",
   "metadata": {},
   "source": [
    "## 2. Carga y exploración del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdeba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ajusta la ruta si fuera necesario\n",
    "csv_path = Path('features_3_sec.csv')\n",
    "raw_df = pd.read_csv(csv_path)\n",
    "print('Dimensiones (intervalos de 3 s):', raw_df.shape)\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeb9ab8",
   "metadata": {},
   "source": [
    "Los archivos de audio están segmentados en **10 intervalos de 3 s**. Cada fila del CSV corresponde a un intervalo. Para reunir la información de cada canción en una sola instancia, se agrupa por el nombre base del archivo y se concatenan las características de los 10 intervalos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716e022",
   "metadata": {},
   "source": [
    "## 3. Agrupación de los 10 intervalos por canción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extraemos el “basename” (género.id) para agrupar intervalos\n",
    "def get_base(filename):\n",
    "    # Ejemplo: blues.00000.3.wav --> blues.00000\n",
    "    return '.'.join(filename.split('.')[:2])\n",
    "\n",
    "raw_df['base'] = raw_df['filename'].apply(get_base)\n",
    "\n",
    "# Columnas numéricas de características\n",
    "feature_cols = [c for c in raw_df.columns if c not in ('filename','label','base')]\n",
    "\n",
    "# Para cada canción unimos sus intervalos: característica_x0, característica_x1, ...\n",
    "rows = []\n",
    "for (base, label), group in raw_df.groupby(['base','label']):\n",
    "    group_sorted = group.sort_values('filename')\n",
    "    row = {}\n",
    "    for idx, (_, row_int) in enumerate(group_sorted.iterrows()):\n",
    "        for col in feature_cols:\n",
    "            row[f'{col}_{idx}'] = row_int[col]\n",
    "    row['label'] = label\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print('Dimensiones (canciones):', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6919e",
   "metadata": {},
   "source": [
    "Ya contamos con **580 características** y la columna `label` con los 10 géneros: *blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, rock*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c337880",
   "metadata": {},
   "source": [
    "## 4. Preprocesamiento: escalado y división train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d9c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Codificación de etiquetas\n",
    "labels = sorted(df['label'].unique())\n",
    "label2idx = {l:i for i,l in enumerate(labels)}\n",
    "df['y'] = df['label'].map(label2idx)\n",
    "\n",
    "X = df.drop(columns=['label','y']).values.astype(np.float32)\n",
    "y = df['y'].values.astype(np.int64)\n",
    "\n",
    "# División estratificada\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=SEED)\n",
    "\n",
    "# Escalado estándar basado solo en training\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da576f5",
   "metadata": {},
   "source": [
    "## 5. Dataset y DataLoader en PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c660199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "test_ds  = TensorDataset(torch.tensor(X_test),  torch.tensor(y_test))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE*2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375eed72",
   "metadata": {},
   "source": [
    "## 6. Definición de modelos autoasociativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0172523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "LATENT_DIM = 64\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=INPUT_DIM, latent_dim=LATENT_DIM):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6bd2f6",
   "metadata": {},
   "source": [
    "Para el *Denoising Auto‑Encoder* se utiliza la misma arquitectura, pero se añade **ruido gaussiano** a la entrada durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cb74b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DenoisingAutoEncoder(AutoEncoder):\n",
    "    def __init__(self, noise_std=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            noise = torch.randn_like(x) * self.noise_std\n",
    "            x = x + noise\n",
    "        return super().forward(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac4a0a",
   "metadata": {},
   "source": [
    "## 7. Funciones auxiliares de entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea47a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_autoencoder(model, loader, epochs=50, lr=1e-3):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    crit = nn.MSELoss()\n",
    "    losses = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(device)\n",
    "            x_hat, _ = model(xb)\n",
    "            loss = crit(x_hat, xb)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epoch_loss += loss.item()*len(xb)\n",
    "        epoch_loss /= len(loader.dataset)\n",
    "        losses.append(epoch_loss)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch:3d}/{epochs} | loss={epoch_loss:.4f}')\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb854d",
   "metadata": {},
   "source": [
    "## 8. Entrenamiento del Auto‑Encoder Estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77066187",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ae = AutoEncoder()\n",
    "ae_losses = train_autoencoder(ae, train_loader, epochs=60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab39a6d",
   "metadata": {},
   "source": [
    "## 9. Entrenamiento del Denoising Auto‑Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25952900",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dae = DenoisingAutoEncoder(noise_std=0.2)\n",
    "dae_losses = train_autoencoder(dae, train_loader, epochs=60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ece596",
   "metadata": {},
   "source": [
    "### Evolución de la pérdida de reconstrucción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3420d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(ae_losses, label='AE')\n",
    "plt.plot(dae_losses, label='DAE')\n",
    "plt.title('Reconstruction loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91befee6",
   "metadata": {},
   "source": [
    "## 10. Extracción de características latentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a4a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_dataset(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Z = model.encoder(torch.tensor(X).to(device)).cpu().numpy()\n",
    "    return Z\n",
    "\n",
    "Z_train_ae = encode_dataset(ae, X_train)\n",
    "Z_test_ae  = encode_dataset(ae, X_test)\n",
    "\n",
    "Z_train_dae = encode_dataset(dae, X_train)\n",
    "Z_test_dae  = encode_dataset(dae, X_test)\n",
    "\n",
    "print('Shape latent:', Z_train_ae.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f549749",
   "metadata": {},
   "source": [
    "## 11. Clasificador MLP sobre el espacio latente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60230761",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=LATENT_DIM, num_classes=len(labels)):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_classifier(model, X_tr, y_tr, X_te, y_te, epochs=40):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    X_tr_t = torch.tensor(X_tr).to(device)\n",
    "    y_tr_t = torch.tensor(y_tr).to(device)\n",
    "    X_te_t = torch.tensor(X_te).to(device)\n",
    "    y_te_t = torch.tensor(y_te).to(device)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        logits = model(X_tr_t)\n",
    "        loss = crit(logits, y_tr_t)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if epoch % 10 == 0:\n",
    "            preds = model(X_te_t).argmax(1)\n",
    "            acc = (preds == y_te_t).float().mean().item()\n",
    "            print(f'Epoch {epoch:2d} | loss={loss.item():.4f} | val_acc={acc:.4f}')\n",
    "    # Final metrics\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = model(X_te_t).argmax(1).cpu().numpy()\n",
    "    return test_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c7a687",
   "metadata": {},
   "source": [
    "### 11.1 MLP sobre representaciones del AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f32d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_ae = MLPClassifier()\n",
    "preds_ae = train_classifier(clf_ae, Z_train_ae, y_train, Z_test_ae, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a065d5",
   "metadata": {},
   "source": [
    "### 11.2 MLP sobre representaciones del DAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a5965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_dae = MLPClassifier()\n",
    "preds_dae = train_classifier(clf_dae, Z_train_dae, y_train, Z_test_dae, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362987f8",
   "metadata": {},
   "source": [
    "## 12. Evaluación y comparación de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c614f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(name, y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f'\\n=== {name} ===')\n",
    "    print('Accuracy:', acc)\n",
    "    print(classification_report(y_true, y_pred, target_names=labels))\n",
    "\n",
    "evaluate('AE + MLP', y_test, preds_ae)\n",
    "evaluate('DAE + MLP', y_test, preds_dae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b2da7",
   "metadata": {},
   "source": [
    "## 13. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac395a1",
   "metadata": {},
   "source": [
    "\n",
    "En este trabajo se compararon dos técnicas de extracción de características basadas en autoencoders:\n",
    "\n",
    "* **Auto‑Encoder Estándar (AE)**\n",
    "* **Denoising Auto‑Encoder (DAE)**\n",
    "\n",
    "Ambos modelos redujeron las 580 variables originales a un espacio latente de 64 dimensiones.  \n",
    "Posteriormente, un MLP sencillo se entrenó sobre estas representaciones para la clasificación de 10 géneros musicales.\n",
    "\n",
    "Los resultados muestran que (completar con el *accuracy* obtenido) el modelo ______ obtiene un rendimiento ligeramente superior/inferior al modelo ______.  \n",
    "En proyectos futuros se podría:\n",
    "\n",
    "* Ajustar el tamaño del *embedding* y la arquitectura del autoencoder.  \n",
    "* Probar **Sparse AE** o **Variational AE**.  \n",
    "* Implementar *data augmentation* a nivel audio en lugar de trabajar sólo con características pre‑calculadas.  \n",
    "* Realizar búsqueda de hiperparámetros y validación cruzada más exhaustiva.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
